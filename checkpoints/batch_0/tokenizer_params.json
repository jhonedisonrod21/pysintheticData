{"vocab_size": 200000, "character_coverage": 1.0, "pretrain_sentence_count": 1000000, "max_line_len": 2048, "tokenizer_type": "SentencePieceTokenizerTrainer", "field_delimiter": ",", "field_delimiter_token": "<d>"}